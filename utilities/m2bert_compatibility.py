#!/usr/bin/env python3
"""
M2-BERT Compatibility Layer
Bridges Poli's original implementation with modern PyTorch
Loads pretrained weights and provides a clean interface
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Optional, Tuple
import math
import json
import os
from dataclasses import dataclass

@dataclass 
class M2BertConfig:
    """Configuration that matches the pretrained model"""
    vocab_size: int = 30528  # Note: not 30522 like standard BERT
    hidden_size: int = 768
    num_hidden_layers: int = 12
    num_attention_heads: int = 12
    intermediate_size: int = 3072
    max_position_embeddings: int = 32768  # 32k context!
    type_vocab_size: int = 2
    hidden_dropout_prob: float = 0.1
    attention_probs_dropout_prob: float = 0.1
    layer_norm_eps: float = 1e-12
    
    # Monarch specific
    use_monarch_mlp: bool = True
    monarch_mlp_nblocks: int = 4
    
    # Hyena operator for long convolutions
    use_hyena: bool = True
    hyena_filter_order: int = 128
    
    @classmethod
    def from_pretrained(cls, model_path: str):
        """Load config from pretrained model directory"""
        config_path = os.path.join(model_path, "config.json")
        with open(config_path) as f:
            config_dict = json.load(f)
        
        # Map from their config to ours
        return cls(
            vocab_size=config_dict.get("vocab_size", 30528),
            hidden_size=config_dict.get("hidden_size", 768),
            num_hidden_layers=config_dict.get("num_hidden_layers", 12),
            num_attention_heads=config_dict.get("num_attention_heads", 12),
            intermediate_size=config_dict.get("intermediate_size", 3072),
            max_position_embeddings=config_dict.get("max_position_embeddings", 32768),
            use_monarch_mlp=config_dict.get("use_monarch_mlp", True),
            monarch_mlp_nblocks=config_dict.get("monarch_mlp_nblocks", 4),
        )


class MonarchLinearCompat(nn.Module):
    """Monarch Linear layer compatible with pretrained weights"""
    
    def __init__(self, in_features: int, out_features: int, nblocks: int = 4, bias: bool = True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.nblocks = nblocks
        
        # For compatibility, we'll use standard linear and reshape
        # This allows us to load pretrained weights more easily
        self.weight = nn.Parameter(torch.empty(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.empty(out_features))
        else:
            self.register_parameter('bias', None)
        
        self.reset_parameters()
    
    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in = self.in_features
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # For now, just use standard linear
        # TODO: Implement actual block-diagonal computation
        return F.linear(x, self.weight, self.bias)


class HyenaOperatorStub(nn.Module):
    """Stub for Hyena operator - processes long-range dependencies"""
    
    def __init__(self, d_model: int, l_max: int = 32768):
        super().__init__()
        self.d_model = d_model
        self.l_max = l_max
        
        # Simplified version - real Hyena uses implicit neural filters
        self.filter_fn = nn.Sequential(
            nn.Linear(5, 128),  # 5 is from positional encoding dimension
            nn.GELU(),
            nn.Linear(128, d_model)
        )
        
        # Learnable bias (found in the pretrained model)
        self.bias = nn.Parameter(torch.zeros(d_model))
        
        # Positional embeddings for Hyena
        self.register_buffer('pos_emb_z', torch.randn(1, l_max, 5))
        self.register_buffer('pos_emb_t', torch.ones(1, l_max, 1))
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Simplified forward - real implementation uses FFT convolution
        return x


class M2BertAttention(nn.Module):
    """M2-BERT attention compatible with pretrained weights"""
    
    def __init__(self, config: M2BertConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = config.hidden_size // config.num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        
        # Use Monarch linear layers if specified
        LinearClass = MonarchLinearCompat if config.use_monarch_mlp else nn.Linear
        
        self.query = LinearClass(config.hidden_size, self.all_head_size)
        self.key = LinearClass(config.hidden_size, self.all_head_size)
        self.value = LinearClass(config.hidden_size, self.all_head_size)
        
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
        
        # Hyena operator for long-range attention
        if config.use_hyena:
            self.filter_fn = HyenaOperatorStub(config.hidden_size, config.max_position_embeddings)
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        batch_size, seq_len, _ = hidden_states.shape
        
        # For very long sequences, use Hyena
        if seq_len > 2048 and hasattr(self, 'filter_fn'):
            # Use Hyena for long-range dependencies
            return self.filter_fn(hidden_states)
        
        # Standard attention for shorter sequences
        # Use modern PyTorch's scaled_dot_product_attention
        query_layer = self.query(hidden_states)
        key_layer = self.key(hidden_states)
        value_layer = self.value(hidden_states)
        
        # Reshape for multi-head
        query_layer = query_layer.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
        key_layer = key_layer.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
        value_layer = value_layer.view(batch_size, seq_len, self.num_attention_heads, self.attention_head_size).transpose(1, 2)
        
        # Use PyTorch's native attention (includes Flash Attention when available)
        attn_output = F.scaled_dot_product_attention(
            query_layer, key_layer, value_layer,
            attn_mask=attention_mask,
            dropout_p=self.dropout.p if self.training else 0.0
        )
        
        # Reshape output
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.view(batch_size, seq_len, self.all_head_size)
        
        return attn_output


class M2BertLayer(nn.Module):
    """Single M2-BERT layer"""
    
    def __init__(self, config: M2BertConfig):
        super().__init__()
        self.attention = M2BertAttention(config)
        self.attention_output = nn.Linear(config.hidden_size, config.hidden_size)
        
        # MLP
        LinearClass = MonarchLinearCompat if config.use_monarch_mlp else nn.Linear
        self.intermediate = LinearClass(config.hidden_size, config.intermediate_size)
        self.output = LinearClass(config.intermediate_size, config.hidden_size)
        
        # Layer norms
        self.attention_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.output_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
    
    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:
        # Self attention
        attention_output = self.attention(hidden_states, attention_mask)
        attention_output = self.attention_output(attention_output)
        attention_output = self.dropout(attention_output)
        hidden_states = self.attention_norm(attention_output + hidden_states)
        
        # MLP
        intermediate_output = F.gelu(self.intermediate(hidden_states))
        layer_output = self.output(intermediate_output)
        layer_output = self.dropout(layer_output)
        layer_output = self.output_norm(layer_output + hidden_states)
        
        return layer_output


class M2BertModel(nn.Module):
    """M2-BERT model compatible with pretrained weights"""
    
    def __init__(self, config: M2BertConfig):
        super().__init__()
        self.config = config
        
        # Embeddings
        self.embeddings = nn.ModuleDict({
            'word_embeddings': nn.Embedding(config.vocab_size, config.hidden_size),
            'position_embeddings': nn.Embedding(config.max_position_embeddings, config.hidden_size),
            'token_type_embeddings': nn.Embedding(config.type_vocab_size, config.hidden_size),
            'LayerNorm': nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps),
        })
        self.embeddings_dropout = nn.Dropout(config.hidden_dropout_prob)
        
        # Encoder
        self.encoder = nn.ModuleList([
            M2BertLayer(config) for _ in range(config.num_hidden_layers)
        ])
        
        # For BERT compatibility
        self.pooler = nn.Linear(config.hidden_size, config.hidden_size)
        
        # Position IDs buffer
        self.register_buffer('position_ids', torch.arange(config.max_position_embeddings).expand((1, -1)))
    
    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None
    ) -> Dict[str, torch.Tensor]:
        batch_size, seq_len = input_ids.shape
        
        # Prepare position IDs
        if position_ids is None:
            position_ids = self.position_ids[:, :seq_len]
        
        # Prepare token type IDs
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        
        # Embeddings
        word_embeds = self.embeddings['word_embeddings'](input_ids)
        position_embeds = self.embeddings['position_embeddings'](position_ids)
        token_type_embeds = self.embeddings['token_type_embeddings'](token_type_ids)
        
        embeddings = word_embeds + position_embeds + token_type_embeds
        embeddings = self.embeddings['LayerNorm'](embeddings)
        embeddings = self.embeddings_dropout(embeddings)
        
        # Attention mask
        if attention_mask is not None:
            # Convert to attention scores format
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
            attention_mask = (1.0 - attention_mask) * -10000.0
        
        # Encoder
        hidden_states = embeddings
        for layer in self.encoder:
            hidden_states = layer(hidden_states, attention_mask)
        
        # Pooler
        pooled_output = self.pooler(hidden_states[:, 0])
        pooled_output = torch.tanh(pooled_output)
        
        return {
            'last_hidden_state': hidden_states,
            'pooler_output': pooled_output
        }


def load_pretrained_m2bert(model_path: str) -> Tuple[M2BertModel, Dict]:
    """Load pretrained M2-BERT model"""
    
    # Load config
    config = M2BertConfig.from_pretrained(model_path)
    
    # Create model
    model = M2BertModel(config)
    
    # Load state dict
    state_dict_path = os.path.join(model_path, "pytorch_model.bin")
    state_dict = torch.load(state_dict_path, map_location='cpu')
    
    # Map weights (handle naming differences)
    mapped_state_dict = {}
    for key, value in state_dict.items():
        # Remove 'bert.' prefix if present
        if key.startswith('bert.'):
            key = key[5:]
        
        # Map embeddings
        if key.startswith('embeddings.'):
            mapped_key = key
        # Map encoder layers
        elif key.startswith('encoder.layer.'):
            # Parse layer number
            parts = key.split('.')
            layer_num = int(parts[2])
            rest = '.'.join(parts[3:])
            
            # Skip Hyena-specific weights for now
            if 'filter_fn' in rest or 'flashfft' in rest:
                continue
            
            # Map attention and MLP weights
            if 'attention.self' in rest:
                rest = rest.replace('attention.self', 'attention')
            
            mapped_key = f'encoder.{layer_num}.{rest}'
        else:
            mapped_key = key
        
        mapped_state_dict[mapped_key] = value
    
    # Load weights (with mismatched keys allowed)
    model.load_state_dict(mapped_state_dict, strict=False)
    
    return model, config


if __name__ == "__main__":
    print("M2-BERT Compatibility Layer")
    print("="*70)
    
    model_path = "./m2_models/models--togethercomputer--m2-bert-80M-32k-retrieval/snapshots/a2ccdc5b5661a282c77545e586a019f387ab7a48"
    
    print(f"Loading from: {model_path}")
    
    try:
        model, config = load_pretrained_m2bert(model_path)
        print(f"âœ“ Model loaded successfully!")
        print(f"  Hidden size: {config.hidden_size}")
        print(f"  Max positions: {config.max_position_embeddings}")
        print(f"  Monarch blocks: {config.monarch_mlp_nblocks}")
        
        # Test forward pass
        device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        model = model.to(device)
        model.eval()
        
        # Test input
        input_ids = torch.randint(0, config.vocab_size, (2, 128), device=device)
        
        with torch.no_grad():
            outputs = model(input_ids)
        
        print(f"\nForward pass successful!")
        print(f"  Output shape: {outputs['last_hidden_state'].shape}")
        print(f"  Pooled shape: {outputs['pooler_output'].shape}")
        
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()